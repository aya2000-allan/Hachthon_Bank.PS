# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1siaxfvnhKObC5ayRUrWMa-O15ZLgzzf4
"""

import pandas as pd

# If your CSV file is encoded in UTF-8 (important for Arabic)
df = pd.read_csv(
    'Dataset.csv',
    encoding='utf-8',
    quotechar='"',
    quoting=0,              # quoting=csv.QUOTE_MINIMAL (default, for normal CSVs with quotes)
    skip_blank_lines=True,
    engine='python'         # engine='python' is needed for multi-line quoted fields
)
print(df)
df = df.sample(frac=1, random_state=42).reset_index(drop=True)

# ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ø®ØªÙ„Ø·Øª
print(df.head())

import re

def clean_arabic_text(text):
    text = re.sub(r"\r\n|\n", " ", text)       # Remove newlines
    text = re.sub(r"http\S+", "", text)        # Remove URLs
    text = re.sub(r"\s+", " ", text).strip()   # Remove extra spaces
    text = re.sub(r"[^Ø¡-ÙŠ\s]", "", text)       # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø±Ù‚Ø§Ù… ÙˆØ§Ù„Ø±Ù…ÙˆØ²
    return text

df['conversation'] = df['conversation'].apply(clean_arabic_text)

def normalize_arabic(text):
    text = re.sub(r"[Ø¥Ø£Ø¢Ø§]", "Ø§", text)  # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø£Ù„Ù
    text = re.sub(r"Ù‰", "ÙŠ", text)       # ØªÙˆØ­ÙŠØ¯ Ø§Ù„ÙŠØ§Ø¡
    text = re.sub(r"Ø¤", "Ùˆ", text)
    text = re.sub(r"Ø¦", "ÙŠ", text)
    text = re.sub(r"Ø©", "Ù‡", text)       # ØªÙˆØ­ÙŠØ¯ Ø§Ù„ØªØ§Ø¡ Ø§Ù„Ù…Ø±Ø¨ÙˆØ·Ø© (Ø£Ùˆ ÙŠÙ…ÙƒÙ† ØªØ¬Ø§Ù‡Ù„Ù‡Ø§)
    text = re.sub(r"Ù‹|ÙŒ|Ù|Ù|Ù|Ù|Ù‘|Ù’", "", text)  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„
    return text
df['conversation'] = df['conversation'].apply(normalize_arabic)

def remove_repeated_letters(text):
    # Ø£ÙŠ Ø­Ø±Ù ÙŠØªÙƒØ±Ø± Ø£ÙƒØ«Ø± Ù…Ù† Ù…Ø±ØªÙŠÙ† ÙŠÙØ­ÙˆÙ„ Ù„Ø­Ø±Ù ÙˆØ§Ø­Ø¯
    return re.sub(r'(.)\1{2,}', r'\1', text)
df['conversation'] = df['conversation'].apply(remove_repeated_letters)

def remove_emojis(text):
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"  # ÙˆØ¬ÙˆÙ‡ ØªØ¹Ø¨ÙŠØ±ÙŠØ©
        "\U0001F300-\U0001F5FF"  # Ø±Ù…ÙˆØ² Ø¹Ø§Ù…Ø©
        "\U0001F680-\U0001F6FF"  # ÙˆØ³Ø§Ø¦Ù„ Ù†Ù‚Ù„
        "\U0001F1E0-\U0001F1FF"  # Ø£Ø¹Ù„Ø§Ù… Ø¯ÙˆÙ„
        "\U00002702-\U000027B0"
        "\U000024C2-\U0001F251"
        "]+",
        flags=re.UNICODE
    )
    return emoji_pattern.sub(r'', text)
df['conversation'] = df['conversation'].apply(remove_emojis)

from sklearn.model_selection import train_test_split

# ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„ØªØ³Ù…ÙŠØ§Øª Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø©
texts = df['conversation'].tolist()
labels = df['label'].map({'Negative': 0, 'Neutral': 1, 'Positive': 2}).tolist()

# Ø£ÙˆÙ„Ù‹Ø§: Ù‚Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ ØªØ¯Ø±ÙŠØ¨ (70%) Ùˆ Ø¨Ø§Ù‚ÙŠ (30%)
train_texts, temp_texts, train_labels, temp_labels = train_test_split(
    texts, labels, test_size=0.3, random_state=42, stratify=labels
)

# Ø«Ø§Ù†ÙŠÙ‹Ø§: Ù‚Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¨Ù‚ÙŠØ© Ø¥Ù„Ù‰ ØªØ­Ù‚Ù‚ (15%) ÙˆØ§Ø®ØªØ¨Ø§Ø± (15%)
# Ø¨Ù…Ø§ Ø£Ù† temp_texts ØªÙ…Ø«Ù„ 30% Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ Ù†Ø£Ø®Ø° Ù†ØµÙÙ‡Ø§ Ù„Ù„ØªØ­Ù‚Ù‚ ÙˆØ§Ù„Ù†ØµÙ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
val_texts, test_texts, val_labels, test_labels = train_test_split(
    temp_texts, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels
)

print(f"Train size: {len(train_texts)}")
print(f"Validation size: {len(val_texts)}")
print(f"Test size: {len(test_texts)}")

!pip install --upgrade transformers
!pip install --upgrade datasets
!pip install --upgrade tokenizers

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from datasets import Dataset
from transformers import TrainingArguments, Trainer

# ØªØ­Ù…ÙŠÙ„ MARBERT
model_name = "UBC-NLP/MARBERT"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)

# ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ Dataset
train_dataset = Dataset.from_dict({'text': train_texts, 'label': train_labels})
val_dataset = Dataset.from_dict({'text': val_texts, 'label': val_labels})
test_dataset = Dataset.from_dict({'text': test_texts, 'label': test_labels})
# Ø¯Ø§Ù„Ø© ØªØ­ÙˆÙŠÙ„ Ù„Ù„Ù…Ø¯Ø®Ù„Ø§Øª
def tokenize(batch):
    return tokenizer(batch['text'], padding='max_length', truncation=True, max_length=128)

train_dataset = train_dataset.map(tokenize, batched=True)
val_dataset = val_dataset.map(tokenize, batched=True)
test_dataset = test_dataset.map(tokenize, batched=True)

# ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ù‡Ù…Ø© ÙÙ‚Ø·
train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])
val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])
test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])

# ØªØ­Ø¯ÙŠØ« pip ÙˆØªØ«Ø¨ÙŠØª Ù†Ø³Ø® Ù…ØªÙˆØ§ÙÙ‚Ø©
!pip install --upgrade pip
!pip install numpy==1.26.4
!pip install transformers==4.40.1 datasets==2.18.0 scikit-learn==1.4.2
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117  # Ù„Ùˆ ØªØ³ØªØ®Ø¯Ù… GPU

!pip install numpy==1.26.4 --quiet
import os
os.kill(os.getpid(), 9)  # Ù„Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†ÙˆØ§Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§

from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset
from sklearn.metrics import accuracy_score, f1_score, recall_score
import numpy as np

# ØªØ¹Ø±ÙŠÙ Ø¯Ø§Ù„Ø© Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
def compute_metrics(p):
    preds = np.argmax(p.predictions, axis=1)
    labels = p.label_ids
    acc = accuracy_score(labels, preds)
    f1 = f1_score(labels, preds, average="macro")
    recall_neg = recall_score(labels, preds, labels=[0], average="macro")  # assuming 0 is negative class
    return {"accuracy": acc, "f1": f1, "recall_negative": recall_neg}

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø¯ÙˆÙ† evaluation_strategy
training_args = TrainingArguments(
    output_dir="./results",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=2,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=10,
    save_strategy="no"   # Ù…Ù†Ø¹ Ø§Ù„Ø­ÙØ¸ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ø£Ù† Ø§Ù„Ù†Ø³Ø®Ø© Ù‚Ø¯ÙŠÙ…Ø©
)

# Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¯Ø±Ø¨
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
    tokenizer=tokenizer
)

# Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
trainer.train()
test_results = trainer.evaluate(eval_dataset=test_dataset)

print(f"âœ… Test Accuracy: {test_results['eval_accuracy']:.4f}")
print(f"âœ… Test F1 Score (macro): {test_results['eval_f1']:.4f}")
print(f"âœ… Test Recall (Negative class): {test_results['eval_recall_negative']:.4f}")

from transformers import AutoTokenizer, AutoModelForSequenceClassification

model_name = "UBC-NLP/MARBERT"

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙˆÙƒÙ†ÙŠØ²Ø± ÙˆØ§Ù„Ù…ÙˆØ¯ÙŠÙ„
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)

# Ø­ÙØ¸ ÙÙŠ Ù…Ø¬Ù„Ø¯ Ù…Ø­Ù„ÙŠ
tokenizer.save_pretrained("./marbert_tokenizer")
model.save_pretrained("./marbert_model")

from datasets import Dataset
import torch
import numpy as np

def predict_texts(texts, model, tokenizer):
    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ
    cleaned_texts = []
    for text in texts:
        text = clean_arabic_text(text)
        text = normalize_arabic(text)
        text = remove_repeated_letters(text)
        text = remove_emojis(text)
        cleaned_texts.append(text)

    # Ø¥Ù†Ø´Ø§Ø¡ Dataset Ù…Ø¤Ù‚Øª Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ
    new_dataset = Dataset.from_dict({'text': cleaned_texts})

    # ØªØ·Ø¨ÙŠÙ‚ Ù†ÙØ³ Ø§Ù„ØªÙˆÙƒÙ†ÙŠØ²Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    def tokenize(batch):
        return tokenizer(batch['text'], padding='max_length', truncation=True, max_length=128)

    new_dataset = new_dataset.map(tokenize, batched=True)

    # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ torch
    new_dataset = new_dataset.with_format("torch", columns=["input_ids", "attention_mask"])

    # ÙˆØ¶Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ ÙˆØ¶Ø¹ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
    model.eval()

    # Ø§Ù„Ù…Ø±ÙˆØ± Ø¹Ù„Ù‰ ÙƒÙ„ Ø¹Ù†ØµØ± ÙˆØ§Ù„ØªÙ†Ø¨Ø¤
    for i in range(len(new_dataset)):
        inputs = {key: val.unsqueeze(0) for key, val in new_dataset[i].items()}  # Ø¥Ø¶Ø§ÙØ© batch dim

        with torch.no_grad():
            outputs = model(**inputs)
            probs = torch.softmax(outputs.logits, dim=1).cpu().numpy()[0]
            pred = np.argmax(probs)

        classes = ["Negative", "Neutral", "Positive"]
        print(f"Ø§Ù„Ù†Øµ: {cleaned_texts[i]}")
        print(f"ğŸŸ© Ø§Ù„ØªÙˆÙ‚Ø¹: {classes[pred]}")
        print("ğŸ”¢ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª:")
        for cls, prob in zip(classes, probs):
            print(f"  {cls}: {prob:.4f}")
        print("-" * 40)

texts_to_test = [
    # Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©
    "Ø§Ù„ØªØ¬Ø±Ø¨Ø© ÙƒØ§Ù†Øª Ø±Ø§Ø¦Ø¹Ø© ÙˆØ³Ø£ÙƒØ±Ø±Ù‡Ø§ Ø¨Ø§Ù„ØªØ£ÙƒÙŠØ¯.",
    "Ø£Ø­Ø¨Ø¨Øª Ø§Ù„Ù…Ù†ØªØ¬ Ø¬Ø¯Ù‹Ø§ØŒ Ø¬ÙˆØ¯Ø© Ù…Ù…ØªØ§Ø²Ø©.",
    "Ø´ÙƒØ±Ø§Ù‹ Ù„Ø®Ø¯Ù…ØªÙƒÙ… Ø§Ù„Ø³Ø±ÙŠØ¹Ø© ÙˆØ§Ù„Ø±Ø§Ø¦Ø¹Ø©.",
    "ÙƒÙ„ Ø´ÙŠØ¡ ÙƒØ§Ù† Ø¹Ù„Ù‰ Ù…Ø§ ÙŠØ±Ø§Ù…ØŒ Ø£Ù†ØªÙ… Ø§Ù„Ø£ÙØ¶Ù„!",

    # Ø³Ù„Ø¨ÙŠØ©
    "Ù„Ù„Ø£Ø³Ù Ø§Ù„Ù…Ù†ØªØ¬ ÙˆØµÙ„ ØªØ§Ù„ÙÙ‹Ø§.",
    "Ø®Ø¯Ù…Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ ÙƒØ§Ù†Øª Ø³ÙŠØ¦Ø© Ø¬Ø¯Ù‹Ø§.",
    "Ù„Ù… ÙŠØªÙ… Ø­Ù„ Ù…Ø´ÙƒÙ„ØªÙŠ Ø±ØºÙ… Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ù…ØªÙƒØ±Ø±.",
    "Ø§Ù„Ø·Ù„Ø¨ ØªØ£Ø®Ø± Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† ÙˆÙ„Ù… ÙŠØªÙ… Ø§Ù„Ø§Ø¹ØªØ°Ø§Ø± Ø­ØªÙ‰!",

    # Ù…Ø­Ø§ÙŠØ¯Ø©
    "Ø§Ø³ØªÙ„Ù…Øª Ø§Ù„Ø·Ø±Ø¯ Ø§Ù„ÙŠÙˆÙ….",
    "Ù„Ù… Ø£Ø¨Ø¯Ø£ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø®Ø¯Ù…Ø© Ø¨Ø¹Ø¯.",
    "Ù„Ø§ Ø£Ø¹Ù„Ù… Ø¥Ù† ÙƒØ§Ù† Ø§Ù„Ù…Ù†ØªØ¬ Ø¬ÙŠØ¯Ù‹Ø§ Ø£Ù… Ù„Ø§.",
    "ÙˆØµÙ„Ù†ÙŠ Ø§Ù„Ø·Ø±Ø¯ ÙƒÙ…Ø§ Ù‡Ùˆ Ù…ÙˆØ¶Ø­ Ø¨Ø§Ù„Ù…ÙˆÙ‚Ø¹.",

    # ØºØ§Ù…Ø¶Ø© Ø£Ùˆ Ø±Ù…Ø§Ø¯ÙŠØ© (Ù„ØªØ­Ø¯ÙŠ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬)
    "Ø§Ù„Ù…ÙˆØ¸Ù ÙƒØ§Ù† Ù„Ø·ÙŠÙÙ‹Ø§ØŒ Ù„ÙƒÙ† Ø§Ù„Ø®Ø¯Ù…Ø© Ø¨Ø·ÙŠØ¦Ø©.",
    "Ø§Ù„Ù…Ù†ØªØ¬ Ø¬ÙŠØ¯ Ù„ÙƒÙ† Ù„ÙŠØ³ ÙƒÙ…Ø§ ØªÙˆÙ‚Ø¹Øª.",
    "Ù‡Ù†Ø§Ùƒ Ø£Ø´ÙŠØ§Ø¡ Ø¬ÙŠØ¯Ø© ÙˆØ£Ø´ÙŠØ§Ø¡ Ø³ÙŠØ¦Ø© ÙÙŠ Ø§Ù„Ø®Ø¯Ù…Ø©.",
    "Ø£Ø¹Ø¬Ø¨Ù†ÙŠ Ø§Ù„ØªØºÙ„ÙŠÙ ÙˆÙ„ÙƒÙ† Ø§Ù„Ø·Ø¹Ù… Ù„Ù… ÙŠÙƒÙ† Ù…Ù…ÙŠØ²Ù‹Ø§.",

    # ØµÙŠØº ØºÙŠØ± Ù…Ø¨Ø§Ø´Ø±Ø©
    "Ù‡Ù„ Ù‡Ø°Ù‡ Ù‡ÙŠ Ø§Ù„Ø®Ø¯Ù…Ø© Ø§Ù„ØªÙŠ ØªÙ‚Ø¯Ù…ÙˆÙ†Ù‡Ø§ØŸ",
    "Ø£ÙØ¶Ù„ Ù„Ùˆ ÙƒØ§Ù†Øª Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø£Ø³Ø±Ø¹.",
    "Ù…Ø§ Ø²Ù„Øª Ø£Ù†ØªØ¸Ø± Ø±Ø¯ÙƒÙ… Ù…Ù†Ø° ÙŠÙˆÙ…ÙŠÙ†.",
    "Ø£Ø¸Ù† Ø£Ù† Ù‡Ù†Ø§Ùƒ Ù…Ø¬Ø§Ù„Ù‹Ø§ Ù„Ù„ØªØ­Ø³ÙŠÙ†."
]

predict_texts(texts_to_test, model, tokenizer)

from collections import Counter
print(Counter(labels))

